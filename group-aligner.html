<!DOCTYPE html>
<html>
<head>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <style>
    .image {
      width: 100%;
      max-width: 1250px; /* or any other preferred value */
      height: auto;
    }
  </style>
  <style>
    .response-item {
      display: flex;
      align-items: center;
      margin-bottom: 10px;
    }

    .option-label {
      margin-right: 10px;
      width: 110px; /* Adjust width based on content */
    }

    .bar-container {
      width: 300px;
      background-color: #f1f1f1;
      margin: 8px 0;
      color: black;
    }

    .bar {
      height: 20px;
      color: white;
      text-align: right;
      padding-right: 5px;
      line-height: 20px; /* Center text vertically */
    }

    .independent { background-color: #808080; width: 50%; } /* Gray */
    .all { background-color: #ffdd59; width: 30%; } /* Yellow */
    .republicans { background-color: #ff6961; width: 10%; } /* Red */
    .democrats { background-color: #779ecb; width: 40%; } /* Blue */

    .independent_11 { background-color: #808080; width: 11%; } /* Gray */
    .independent_28 { background-color: #808080; width: 28%; } /* Gray */
    .independent_30 { background-color: #808080; width: 30%; } /* Gray */
    .independent_31 { background-color: #808080; width: 31%; } /* Gray */

    .all_14 { background-color: #ffdd59; width: 14%; } /* Yellow */
    .all_27 { background-color: #ffdd59; width: 27%; } /* Yellow */
    .all_29 { background-color: #ffdd59; width: 29%; } /* Yellow */
    .all_30 { background-color: #ffdd59; width: 30%; } /* Yellow */

    .republicans_17 { background-color: #ff6961; width: 17%; } /* Red */
    .republicans_22 { background-color: #ff6961; width: 22%; } /* Red */
    .republicans_39 { background-color: #ff6961; width: 39%; } /* Red */

    .democrats_5 { background-color: #779ecb; width: 5%; } /* Blue */
    .democrats_15 { background-color: #779ecb; width: 15%; } /* Blue */
    .democrats_33 { background-color: #779ecb; width: 33%; } /* Blue */
    .democrats_47 { background-color: #779ecb; width: 47%; } /* Blue */

    .legend {
      display: flex;
      justify-content: space-around;
      margin-top: 20px;
    }

    .legend-box {
      display: flex;
      align-items: center;
      margin-right: 10px;
    }

    .color-box {
      width: 20px;
      height: 20px;
      margin-right: 5px;
    }

    pre {
      font-family: 'Courier New', monospace;
      background-color: #1e1e1e;
      border: 0.5px solid #3c3c3c;
      overflow: auto;
    }

    .side-by-side-container {
      display: flex;
    }
    .side-by-side-container figure {
      width: 33%; /* Adjust the width as necessary */
      margin: 0px;
      display: flex;
      flex-direction: column;
    }
    .side-by-side-container img {
      width: 100%;
    }
    .side-by-side-container figcaption {
      text-align: center;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
    }
    th, td {
      padding: 12px;
      border: 1px solid #ccc;
      text-align: left;
    }
    th {
      background-color: #f2f2f2;
      color: #333;
    }
    tr:nth-child(even) {
      background-color: #f9f9f9;
    }
    tr:hover {
      background-color: #f1f1f1;
    }
    .democrat {
      color: #779ecb;
    }
    .republican {
      color: #ff6961;
    }
    .overall {
      color: #ffdd59;
    }
  </style>
  <meta charset="utf-8">
  <meta name="description"
        content="Group Preference Optimization: Few-Shot Alignment of Large Language Models.">
  <meta name="keywords" content="Group Preference Optimization, GPO, LLM, Large Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Group Preference Alignment</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

  <script>
    document.addEventListener('DOMContentLoaded', function () {
      // Find all list items in the references section
      var references = document.querySelectorAll('#References ol li');
      // Loop through each citation link
      document.querySelectorAll('a.citation-link').forEach(function(link) {
        // Get the ID from the href attribute
        var refId = link.getAttribute('href').substring(1);
        // Find the corresponding reference list item by ID
        var refItem = document.getElementById(refId);
        if (refItem) {
          // Get the index of the reference
          var refIndex = Array.prototype.indexOf.call(references, refItem) + 1;
          // Set the index as the text of the superscript
          link.textContent = '[' + refIndex + ']';
        }
      });
    });
  </script>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Group Preference Aligner</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/chanukyavardhan/">Chanukya Vardhan Gujjula</a><sup>1</sup>,
              <a href="https://www.linkedin.com/in/sai-charitha-akula-32574887/">Sai Charitha Akula</a><sup>1</sup>
            </span>
           
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>New York University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ChanukyaVardhan/group-aligner"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Advancements in Large Language Models (LLMs) have highlighted the critical need for aligning their outputs with the diverse preferences of user groups. The risks of bias and misrepresentation should be addressed. Traditional methods such as supervised fine-tuning and strategic prompting often fall short in terms of efficiency or effectiveness. Our research introduces a novel Group Preference Aligner (GPA) model that significantly enhances group-specific alignment by leveraging a two-stage process. Initially, we use a universal aligner to adjust LLM outputs towards universally beneficial attributes like safety and harmlessness. Subsequently, we fine-tune this model using only adapter modules tailored to the specific preferences of individual groups, enhancing the LLM's ability to cater to nuanced group characteristics, while increasing the parameters marginally. We introduce a new synthetic DescriptiveOpinionQA dataset to model more descriptive group preferences beyond multiple-choice options in the OpinionQA dataset. We demonstrate the effectiveness of our approach using the OpinionQA and our synthetic DescriptiveOpinionQA datasets. Our findings show that the GPA model aligns more closely with group-specific preferences compared to existing models.
            </p>
          </div>

<hr>
        <h2 class="title is-3">Background</h2>
          <div class="content has-text-justified">
              <p>LLMs frequently exhibit biased representations, disproportionately emphasizing or neglecting certain groups. Directive prompts like <i>"Speak as [specific identity or role]"</i> offer a partial solution by providing contextual cues, yet they risk misrepresentation and have inherent limitations. Historically, methods such as supervised fine-tuning, strategic prompting, and learning from contextual examples have been used to align LLM outputs with user or group preferences. These methods, however, often prove to be inefficient or ineffective.</p>
              
              <h3 class="title is-4">Group Preference Optimization</h3>
                <p>The recent Group Preference Optimization (GPO)<a href="#gpo" class="citation-link"></a> approach tries to address these challenges. GPO leverages a framework that aligns LLM outputs with the preferences of various groups using a few-shot learning paradigm. This method uses an auxiliary transformer module that predicts the preferences of a group based on the outputs of a base LLM among the options provided.</p>
                
                <p>Group alignment in GPO aims to steer pretrained LLMs to preferences catering to a wide range of groups. For each group <em>g</em>, the preference dataset is represented as <em>D<sub>g</sub></em> = {(<em>x<sub>1</sub><sup>g</sup></em>, <em>y<sub>1</sub><sup>g</sup></em>), ..., (<em>x<sub>n</sub><sup>g</sup></em>, <em>y<sub>n</sub><sup>g</sup></em>)}. Here, <em>y<sub>i</sub><sup>g</sup></em> signifies the preference of group <em>g</em> for a pair of given prompt <em>q<sub>i</sub><sup>g</sup></em> and response <em>r<sub>i</sub><sup>g</sup></em>, while <em>x<sub>i</sub><sup>g</sup></em> is its LLM representation obtained with π<sub>emb</sub>(<em>q<sub>i</sub><sup>g</sup></em>, <em>r<sub>i</sub><sup>g</sup></em>), as shown in Figure <a href="#figure1">1</a>.</p>

                <figure id="figure1">
                  <img src="images/gpo_example.png" alt="GPO dataset" style="width:80%">
                  <figcaption>Figure 1: Group Preference Datasets</figcaption>
                </figure>

                <p>As shown in Figure <a href="#figure2">2</a> and <a href="#figure3">3</a>, a set of few known input-output pairs from a group, (<em>x<sub>1</sub></em>, <em>y<sub>1</sub></em>), (<em>x<sub>2</sub></em>, <em>y<sub>2</sub></em>), ..., (<em>x<sub>m</sub></em>, <em>y<sub>m</sub></em>) are provided as the context points along a base prompt. These provide the model with known preferences for the given inputs. Preferences <em>y&#770;<sub>m+1</sub></em>, ..., <em>y&#770;<sub>n</sub></em> for new inputs (<em>x<sub>m+1</sub></em>, 0), ..., (<em>x<sub>n</sub></em>, 0) are predicted based on the patterns learned from the context points.</p>

                <figure id="figure2">
                  <img src="images/gpo_method.png" alt="GPO method" style="width:100%">
                  <figcaption>Figure 2: Illustration of GPO Architecture</figcaption>
                </figure>
                <figure id="figure3">
                  <img src="images/gpo_inference.png" alt="GPO inference" style="width:80%">
                  <figcaption>Figure 3: Illustration of GPO Architecture</figcaption>
                </figure>
                
                <p>Despite its advantages, GPO has limitations. GPO predicts distribution of options for a given question and not the direct answer. This makes it difficult to directly use it as an LLM or as an extension to LLM. GPO is a few-shot paradigm, depending heavily on the context and thus can struggle with longer context lengths.</p>

              <h3 class="title is-4">Aligner</h3>
                <p>Parallel to GPO, there is another line of work: Aligner<a href="#aligner" class="citation-link"></a>. The Aligner model represents a novel approach to aligning LLMs with human opinions without the need for Reinforcement Learning from Human Feedback (RLHF) processes. It doesn't rely on reward model training and actor-critic engineering. It operates on the principle of learning correctional differences between aligned and unaligned responses directly from the data, structured as an autoregressive sequence-to-sequence (seq2seq) model trained on query-answer-correction (Q-A-C) triples. Given a question and output of an LLM as Answer, Aligner predicts the potential correction that is more aligned to safety and harmlessness.</p>

                <p>The Aligner model stacks upon an upstream LLM. This model corrects the answers of the upstream LLM's output and redistributes the initial answers, thus aligning the composed LLM responses towards the aligned distribution. Aligner takes the user’s query <em>x</em> and the initial answer <em>y<sub>o</sub></em> generated by the upstream LLM, then generates the answer <em>y<sub>c</sub></em> which is better aligned as required. The seq2seq model is trained to redistribute the preliminary answers <em>y<sub>o</sub></em> to the aligned answer <em>y<sub>c</sub></em> as shown in Figure <a href="#figure4">4</a> and <a href="#figure5">5</a>.</p>
                <figure id="figure4">
                  <img src="images/aligner_method.jpg" alt="Aligner method" style="width:80%">
                  <figcaption>Figure 4: Architecture of Aligner Module</figcaption>
                </figure>
                <figure id="figure5">
                  <img src="images/aligner_train.png" alt="Aligner train" style="width:80%">
                  <figcaption>Figure 5: Illustration of Aligner Architecture</figcaption>
                </figure>

                <p>Aligner can be integrated with any pre-existing LLM making it model agnostic. This Plug-and-Play capability enhances its alignment capabilities without the need for direct modifications to the underlying LLM. Aligner demonstrates significant improvements in metrics such as helpfulness and harmlessness, achieving these gains with lower computational demands compared to traditional RLHF methods.</p>
              
              <h3 class="title is-4">Group Preference Aligner</h3>
                <p>Building upon the foundational work of the Aligner model, we introduce an innovative extension, Group Preference Aligner (GPA), that significantly enhances group-specific alignment for LLMs. While Aligner focuses on adjusting LLM outputs to generate safer and more harmless responses universally, our Group Aligner is also tailored to the nuances and preferences of individual groups. Specifically, we propose a two-stage alignment process: The first stage involves training a universal aligner as mentioned in the aligner paper to address common preferences across groups (such as safety and harmlessness) and the second stage involves fine-tuning the model by incorporating adapter modules to align with the nuanced preferences of individual groups.</p>

                Comparative Advantages of Group Preference Aligner:
                <ul>
                    <li><strong>Robustness and Context Length:</strong> Unlike GPO, our method support descriptive conversations, and doesn't require longer contexts.</li>
                    <li><strong>Efficiency in Group-Specific Preferences:</strong> While the first stage has high parameters, our method efficiently manages individual group adaptations with significantly fewer parameters than full fine-tuning approaches, making it highly effective and scalable.</li>
                    <li><strong>Plug-and-Play Flexibility:</strong> Retaining Aligner’s plug-and-play capability, once trained, our Group Aligner module can be potentially added to any LLM to refine outputs, effectively enhancing model versatility and applicability across various applications.</li>
                </ul>
          </div>

<hr>

        <h2 class="title is-3" style="text-align: center;">Data</h2>
          <div class="content has-text-justified">
            <h3 class="title is-4">Datasets</h3>
              <p>We use the below two datasets.</p>
              <ul>
                <li>
                  <strong>Survey Dataset - OpinionQA</strong>: OpinionQA<a href="#opinionqa" class="citation-link"></a> is a recent survey dataset designed to facilitate research in opinion-based question answering systems. It comprises a diverse collection of survey questions sourced from real-world data, capturing a wide range of opinions across various demographics. OpinionQA spans 22 US demographic groups (e.g., income, political ideology, race, and sex) across 500 multiple-choice questions.
                </li>
                <li>
                  <strong>Synthetic Dataset - DescriptiveOpinionQA</strong>: Since OpinionQA is constructed based on the survey questions which have options, we looked out for aligning conversations that have better descriptive answers rather than simple options. To achieve this, we construct a synthetic dataset using GPT-3.5-Turbo to generate a descriptive version for the same questions available in the OpinionQA dataset. We use prompts similar to those used in steering the LLM in OpinionQA to generate descriptive responses as alternatives to the options provided in the survey question of OpinionQA.
                </li>
              </ul>

            <p>The datasets are processed to get to question-answer-correction (Q-A-C) format following Aligner<a href="#aligner" class="citation-link"></a>. Q is the question that is directly picked from OpinionQA. A is the overall distribution answer. C is the corrected response that the Aligner is supposed to generate and align the answer A to the respective group.</p>
            <br>

            <h4 class="title is-5">OpinionQA</h4>
              <p>An example question answer distribution from the dataset is shown below.</p>
              <strong>Question:</strong> How much, if at all, do you think the ease with which people can legally obtain guns contributes to gun violence in the country today?
              <ol type="A">
                <li>A great deal</li>
                <li>A fair amount</li>
                <li>Not too much</li>
                <li>Not at all</li>
              </ol>
              <strong>Response Distribution</strong>
              <div class="response-item">
                  <span class="option-label">A great deal</span>
                  <div class="bar-container">
                      <div class="bar all_30">30%</div>
                      <div class="bar democrats_47">47%</div>
                      <div class="bar republicans_17">17%</div>
                      <div class="bar independent_28">28%</div>
                  </div>
              </div>
              <div class="response-item">
                  <span class="option-label">A fair amount</span>
                  <div class="bar-container">
                      <div class="bar all_29">29%</div>
                      <div class="bar democrats_33">33%</div>
                      <div class="bar republicans_22">22%</div>
                      <div class="bar independent_31">31%</div>
                  </div>
              </div>
              <div class="response-item">
                  <span class="option-label">Not too much</span>
                  <div class="bar-container">
                      <div class="bar all_27">27%</div>
                      <div class="bar democrats_15">15%</div>
                      <div class="bar republicans_39">39%</div>
                      <div class="bar independent_30">30%</div>
                  </div>
              </div>
              <div class="response-item">
                  <span class="option-label">Not at all</span>
                  <div class="bar-container">
                      <div class="bar all_14">14%</div>
                      <div class="bar democrats_5">5%</div>
                      <div class="bar republicans_22">22%</div>
                      <div class="bar independent_11">11%</div>
                  </div>
              </div>
              <div class="legend">
                  <div class="legend-box">
                      <div class="color-box" style="background-color: #ffdd59;"></div>
                      <span>Overall</span>
                  </div>
                  <div class="legend-box">
                      <div class="color-box" style="background-color: #779ecb;"></div>
                      <span>Democrats</span>
                  </div>                  
                  <div class="legend-box">
                      <div class="color-box" style="background-color: #ff6961;"></div>
                      <span>Republicans</span>
                  </div>
                  <div class="legend-box">
                      <div class="color-box" style="background-color: #808080;"></div>
                      <span>Independent</span>
                  </div>
              </div>

              <br>


              <p>Since the OpinionQA dataset provides distribution statistics of the options selected at the group level, and the overall statistics; we select the answer with high probability from the overall group as the answer from the LLM. The correction for each group is the option with high probability from the answer distribution for the respective question. This would let Aligner correct the responses to generate group opinions.</p>

              <p>An example question-answer-correction generated for Democrat and Republican groups is shown below.</p>
              <strong>Question:</strong> How much, if at all, do you think the ease with which people can legally obtain guns contributes to gun violence in the country today?<br>
              <strong style="color: #ffdd59; font-weight: bold;">Overall Answer:</strong> A great deal<br>
              <strong style="color: #779ecb; font-weight: bold;">Correction (Democrat):</strong> A great deal<br>
              <strong style="color: #ff6961; font-weight: bold;">Correction (Republican):</strong> Not too much<br>
              <strong style="color: #808080; font-weight: bold;">Correction (Independent):</strong> A fair amount<br><br>

            <h4 class="title is-5">DescriptiveOpinionQA</h4>
              <p>To generate better descriptive responses, we leveraged GPT-3.5-Turbo and constructed synthetic answers and corrections for each group. We use the following group level prompts and get a response for each question available in the OpinionQA dataset.</p>
              <pre>group_intepretation_map = {
  "Overall": "Consider yourself as a United States citizen.",
  "Democrat": "Consider yourself as a United States citizen. In today's United States politics, you are Democrat.",
  "Republican": "Consider yourself as a United States citizen. In today's United States politics, you are Republican.",
  "$100,000 or more": "Consider yourself as a United States citizen. Your total family annual income from all sources before taxes is $100,000 or more.",
  "Less than $30,000": "Consider yourself as a United States citizen. Your total family annual income from all sources before taxes is less than $30,000."
}</pre>

              <p>Data generation follows this sequence:</p>
              <ol>
                <li>Set the appropriate group context based on the group interpretation map.</li>
                <li>Generate responses using GPT-3.5-Turbo by feeding a question and interpreting the context.</li>
                <li>Store the generated responses for each group to all the questions.</li>
              </ol>

              <pre>
system_prompt = f"""
    {group_intepretation_map[Group]} 
    Answer this survey question from your perspective in a brief sentence.
    Don't start the answer with your description or group affiliation.
    The answer should reflect your concerns and life experiences relevant to the topic."
"""
user_prompt = f"Question: {Question}."

chat_completion = openai_client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ],
    temperature=0.5
)
response = chat_completion.choices[0].message.content</pre>

              <p>The responses generated for the question in the <i>Overall</i> group is considered as the default response (answer) from the language model, followed by the responses for individual groups as the corrections to the overall answers.</p>

              <p>An example question-answer-correction generated for Democrat and Republican groups is shown below.</p>
              <strong>Question:</strong> How much, if at all, do you think the ease with which people can legally obtain guns contributes to gun violence in the country today?<br>
              <strong style="color: #ffdd59; font-weight: bold;">Overall Answer:</strong> I believe that the ease with which people can legally obtain guns in the United States does contribute to gun violence, and I am concerned about the impact it has on the safety of our communities.<br>
              <strong style="color: #779ecb; font-weight: bold;">Correction (Democrat):</strong> The ease of legally obtaining guns in the United States definitely contributes to the high rates of gun violence we see in our country today, and stricter gun control measures are necessary to address this issue.<br>
              <strong style="color: #ff6961; font-weight: bold;">Correction (Republican):</strong> I believe that the ease of legally obtaining guns should be balanced with ensuring proper background checks and mental health evaluations to prevent gun violence in our country.<br>

              <h3 class="title is-4">Discussion on Chosen Groups</h3>
              <p>For our project, we chose specific groups such as income (Less than $30,000, $100,000 or more) and political orientation(Democrat, Republican) based on their disparate nature, as demonstrated in the OpinionQA data analysis.</p>

              <!-- <h3 class="title is-4">Exploring Option Descriptions</h3>
              <p>Despite generating data using available options shown to GPT, results were unsatisfactory.</p> -->
          </div>
          
<hr>
          <h2 class="title is-3">Method</h2>
            <div class="content has-text-justified">
              <h3 class="title is-4">Group Preference Aligner</h3>
                <p>
                  Our method builds on the Aligner model. It has two stages.</p>
                <ol>
                  <li><strong>Universal Alignment:</strong> Initially, a universal aligner model is trained to address common preferences across groups, focusing on universally applicable aspects such as safety and harmlessness. For this project, we used a pretrained <i>Aligner-7B</i> <a href="#aligner-7b" class="citation-link"></a> model. This model was pretrained on 20K aligner (Q-A-C) dataset<a href="#aligner-20k" class="citation-link"></a> that was constructed using prompted models like GPT-4, LLama2-70B-Chat and also involving human annotators ensuring the corrections align with established principles emphasizing safety, helpfulness, and harmlessness.</li>
                  <!-- For this project, we used a pretrained Aligner model (7B parameters). This Aligner model is a result of finetuning Alpaca model on 20k training data generated by GPT-4 (Verify and add more details).  -->
                  <li><strong>Group-Specific Fine-Tuning:</strong> The above model is fine-tuned by integrating adapter modules tailored to the specific preferences of individual groups, allowing for precise alignment with nuanced group characteristics. We work with two different settings.
                    <ul>
                      <li>In the first case, we take the Aligner model with a single LoRA adapter module and finetune it with data from all the groups.</li>
                      <li>In the second case, we finetune separate LoRA adapter modules for each group.</li>
                    </ul>
                  We train the adapters on the generated DescriptiveOpinionQA dataset, which has (Q, A, C) datapoints for each group as expected by Aligner. We update the default helpful and harmless correction prompt used in Aligner to the group-specific prompt by instructing the model to generate responses from the viewpoint of an individual from the group. This is changing the correction prompt from <i>"Edit the following Question-Answer pair to make it more helpful and harmless:"</i> to <i>"Edit the following Question-Answer pair to make it more aligned to a Democrat's view:"</i> for the Democrat group.
                </ol>
                At the end, we will have one base model and a single adapter module or one base model and multiple adapter modules for each group.

              <h3 class="title is-4">Comparison to GPO</h3>
                Since GPO requires options to be passed along with the question as an input, and it also predicts the option distribution rather than generating the answer directly, this is not directly comparable to our GPA methodology. Along with this, GPO doesn't have any base model that is trained on safety data. For a better and fair comparison of GPO with our method, we adopted the following training strategies for GPA and GPO.
                <ol>
                  <li><strong>GPA-Objective:</strong> We use the same training strategy for GPA as mentioned earlier, but now train it to predict the options available in OpinionQA instead of generating completely descriptive responses. This means we train another GPA model directly on the OpinionQA dataset instead of our generated DescriptiveOpinionQA dataset. For the (Q, A, C) training pairs, the survey question is given as the Q, the answer from the overall distribution with maximum distribution is the A, and the group-specific answer with maximum probability as the correction C. This model would still take only the question as input and not all the options, unlike GPO, but the generated output will be trained to match the option answer instead of a descriptive answer.</li>
                  <li><strong>S-GPO (Safe GPO):</strong> Our GPA has a universal alignment stage first that aligns the LLM model towards safety and harmlessness. Following the same methodology, choosing the base LLM that is trained on safety and harmlessness as the first stage before GPO would make the comparison even. The Aligner model<a href="#aligner" class="citation-link"></a> was compared to a finetuned alpaca model following the DPO<a href="#dpo" class="citation-link"></a> and PPO<a href="#ppo" class="citation-link"></a> methodologies trained on the same Aligner dataset. So, we use this DPO finetuned alpaca model as the universal alignment stage before GPO. However, the checkpoints for these are not made available. We replicated the same by finetuning alpaca-7b model on aligner-20K safety dataset using DPO, which serves as the base for the second stage. For the second stage, we now train GPO on this DPO finetuned alpaca model using OpinionQA dataset following the same training strategy used in GPO<a href="#gpo" class="citation-link"></a>. For final comparison to GPA-Objective, we convert the distribution of options that's generated by the S-GPO model to a single answer by taking the option that has the highest probability.</li>
                </ol>

                <p>By following the above changes to training strategies to start from a similar first stage base, the second stage training should give a good idea about the differences among the GPA and GPO approaches.</p>

                <!-- In summary, while the original Aligner model provides a robust foundation for generating safer and more responsible outputs from LLMs, our Group Aligner innovation extends this capability to accommodate the specific needs of diverse groups, offering a more nuanced, respectful, and effective interaction model that is particularly suited to large-scale, diverse applications.-->
            </div>

<hr>
          <h2 class="title is-3">Results</h2>
            <div class="content has-text-justified">
              <h3 class="title is-4">Experimental Setup</h3>
                <p>We use NVIDIA Quadro RTX 8000 and NVIDIA A100 GPUs available on NYU Greene HPC. To finetune Aligner models on OpinionQA and DescriptiveOpinionQA we use the aligner codebase<a href="#aligner-code" class="citation-link"></a>. We use the same codebase to finetune alpaca-7b on the aligner-20K dataset using DPO. To train GPO we follow the code provided at <a href="#gpo-code" class="citation-link"></a>.</p>
                Specifically,
                <ul>
                  <li> We use aligner codebase<a href="#aligner-code" class="citation-link"></a> for </li>
                    <ul>
                      <li>Second stage of both GPA-Objective and GPA-Desriptive models.</li>
                      <li>First stage of S-GPO: Finetuning alpaca-7b on the aligner-20K dataset using DPO.</li>
                    </ul>
                  <li>We use GPO codebase<a href="#gpo-code" class="citation-link"></a> for Second stage of S-GPO: Finetuning s-alpaca on Opinion QA using GPO (S-GPO).</li>
                </ul>

              <h3 class="title is-4">Evaluation Metrics</h3>
                <p>We evaluate the performance of all the trained models through qualitative and quantitative estimates. For qualitative evaluation, we manually evaluate generated responses on the evaluation set and compare its alignment with the correction compared to the answer. For quantitative evaluation, we chose the Rouge metric due to its relevance in assessing the quality of descriptive outputs. The Rouge metric measures the overlap of n-grams between the generated responses and a set of reference responses, providing a robust indicator of textual similarity. It is important to note, however, that while the ROUGE metric offers a useful estimate of similarity, it is not the ideal metric for evaluating all aspects of response quality. Particularly for more descriptive answers, this value may be lower, indicating that ROUGE can sometimes fail to capture the nuanced content and creativity of the responses.</p>

              <h3 class="title is-4">GPA-Descriptive</h3>
                <p>Following are some qualitative examples from the individual adapters:</p>
                <strong>Question:</strong> Thinking about long-range foreign policy goals, how much priority, if any, do you think reducing legal immigrations into the US should be given?
                <table border="2" style="border-collapse: collapse; border-radius: 10px; overflow: hidden;">
                    <tr>
                        <th style="text-align: center;"></th>
                        <th style="text-align: center; color: #779ecb;">Democrat</th>
                        <th style="text-align: center; color: #ff6961;">Republican</th>
                        <th style="text-align: center; color: #ffdd59;">Overall Answer</th>
                    </tr>
                    <tr>
                        <td style="text-align: center; font-weight: bold;">Groundtruth Correction</td>
                        <td>Reducing legal immigration should not be a top priority in long-range foreign policy goals; instead, the focus should be on comprehensive immigration reform that promotes fairness, security, and economic growth.</td>
                        <td>Reducing legal immigration should be a significant priority in our long-range foreign policy goals to ensure the protection of American jobs and national security.</td>
                        <td>Reducing legal immigration should not be a top priority in long-range foreign policy goals. It is important to balance national security concerns with the contributions and diversity that immigrants bring to the United States.</td>
                    </tr>
                    <tr>
                        <td style="text-align: center; font-weight: bold;">Generated Correction</td>
                        <td>Reducing legal immigration should not be a top priority in long-range foreign policy goals; instead, I believe in inclusive and fair immigration policies that benefit the United States and all its citizens.</td>
                        <td>Reducing legal immigration should be a top priority in order to protect American jobs and culture.</td>
                        <td></td>
                    </tr>
                </table>

                <strong>Question:</strong> How much, if at all, do you think the following proposals would do to reduce economic inequality in the U.S.? Expanding government benefits for the poor.
                <table border="2" style="border-collapse: collapse; border-radius: 10px; overflow: hidden;">
                    <tr>
                        <th style="text-align: center;"></th>
                        <th style="text-align: center; color: #779ecb;">Less than $30,000</th>
                        <th style="text-align: center; color: #ff6961;">$100,000 or more</th>
                        <th style="text-align: center; color: #ffdd59;">Overall Answer</th>
                    </tr>
                    <tr>
                        <td style="text-align: center; font-weight: bold;">Groundtruth Correction</td>
                        <td>Expanding government benefits for the poor would greatly help reduce economic inequality in the U.S. as it would provide much-needed support for low-income families like mine to meet basic needs and have a better chance at financial stability.</td>
                        <td>Expanding government benefits for the poor could help reduce economic inequality by providing necessary support for those in need, but it is important to ensure that these programs are efficient and effective in targeting those who truly need assistance.</td>
                        <td>Expanding government benefits for the poor would help reduce economic inequality by providing necessary support for those in need and helping to level the playing field for all citizens.</td>
                    </tr>
                    <tr>
                        <td style="text-align: center; font-weight: bold;">Generated Correction</td>
                        <td>Expanding government benefits for the poor would greatly help reduce economic inequality by providing much-needed support to families like mine who are struggling to make ends meet due to limited income.</td>
                        <td>Expanding government benefits for the poor would definitely help reduce economic inequality by providing much-needed support and opportunities for upward mobility for those struggling to make ends meet.</td>
                        <td></td>
                    </tr>
                </table>

                <p>From the above examples we can see that the generated corrections for each of the groups follow the expected tone as in their groundtruth response.</p>

                <p>Rouge scores for GPA trained on DescriptiveOpinionQA dataset for individual adapters trained separately for each group and a common adapter trained collectively for all groups can be seen in the figure below. We follow a train-test split of 90%-10%.</p>
                <!-- <figure id="gpa_descriptive_rouge1">
                    <img src="images/gpa_descriptive_rouge1.png" alt="GPA Descriptive Rouge 1" style="width:80%">
                    <figcaption>GPA-Descriptive: Rouge 1</figcaption>
                </figure>
                <figure id="gpa_descriptive_rouge2">
                    <img src="images/gpa_descriptive_rouge2.png" alt="GPA Descriptive Rouge 2" style="width:80%">
                    <figcaption>GPA-Descriptive: Rouge 2</figcaption>
                </figure>
                <figure id="gpa_descriptive_rougel">
                    <img src="images/gpa_descriptive_rougel.png" alt="GPA Descriptive Rouge l" style="width:80%">
                    <figcaption>GPA-Descriptive: Rouge l</figcaption>
                </figure> -->
                <figure id="gpa_descriptive">
                    <img src="images/gpa_descriptive.png" alt="GPA Descriptive" style="width:80%">
                    <figcaption>GPA-Descriptive Rouge Scores</figcaption>
                </figure>
                <p>The results indicate comparable performance between the single adapter setting and individual adapters, suggesting that adding more parameters could further enhance the results.</p>

              <h3 class="title is-4">Comparative Analysis</h3>
                We compare the performance of GPA-Objective with GPO methods and the quantitative results for these as shown below. We see the rouge score of GPA-Objective is better than S-GPO.
                <figure id="comparison_gpa_gpo">
                    <img src="images/comparison_gpa_gpo.png" alt="Comparison GPA GPO" style="width:80%">
                    <figcaption>GPA-Objective vs GPO Comparison Results</figcaption>
                </figure>

              <h3 class="title is-4">Key Highlights</h3>
                <p><strong>Training and Evaluation Dynamics of GPO:</strong> When training GPO by splitting groups into train and eval (90% of groups training, 10% of groups evaluation), the rouge scores and alignment scores (of the predicted distributions, calculated using Wasserstein Distance) are notably high in comparison to taking eval split from all the groups, for both with 22 groups and with a subset of groups. This suggests potential overfitting to familiar questions. The below plots showing the rouge score and alignment score clearly highlights this.</p>
                <figure id="gpo_rougel">
                    <img src="images/gpo_rougel.png" alt="GPO rougel" style="width:80%">
                    <figcaption>GPO Rouge L Score GroupSplit vs EvalSplit</figcaption>
                </figure>
                <figure id="gpo_alignment">
                    <img src="images/gpo_alignment.png" alt="GPO alignment" style="width:80%">
                    <figcaption>GPO Alignment Score GroupSplit vs EvalSplit</figcaption>
                </figure>

                <p><strong>Classification vs. Regression Discrepancies:</strong> The evaluation may inherently favor the GPO model as it predicts a probability distribution over options rather than generating direct text. This setup tends to yield higher Rouge scores for GPO, where direct text generation introduces more variability and challenge in matching the exact content of options.</p>

                <p><strong>Adaptation to New Groups:</strong> Scalability poses a challenge across various alignment models, including our own. Adding a new group to our model necessitates retraining the adapter module, introducing scalability concerns. However, this overhead is manageable, especially when considering the overall benefits of precise group alignment and the fact that the added parameters are minimal compared to the actual size of the Aligner model. While GPO significantly reduces some aspects of scalability issues, it still incurs a small overhead in managing long prompts. It requires the pre-computation of embeddings for all Q&As at the outset. During inference, this necessitates generating context by appending embeddings for a selected number of Q-As each time a new group is assessed. Despite these challenges, both models manage the added complexity with relatively low overhead, maintaining usability and effectiveness across various settings.</p>

                <!-- <p><strong>Compute and Parameter Efficiency:</strong> REFINE AND ADD MORE
                Parameter Impact: Each LoRA adapter we introduce adds a marginal percentage increase to the base model's 7 billion parameters.
                Compute Considerations: Should we also discuss the training time and computational resources required? -->
            </div>

<hr>
          <h2 class="title is-3">Challenges</h2>
            <div class="content has-text-justified">
              <h3 class="title is-4">Data Generation Challenges</h3>
                <p><strong>Inconsistent Output Formats:</strong> The outputs from LLMs did not adhere to a common format, necessitating extensive post-processing to shape the data into a usable form for training. This involved significant cleaning and manipulation to meet the desired data format. </p>
                <p><strong>Temperature Tuning:</strong> Adjusting the generation temperature of the LLM was crucial to balance creativity and relevance in the responses, ensuring that the data generated met our expectations.</p>
                <p><strong>Prompt Engineering:</strong> Crafting the right prompts was a big challenge. We needed to avoid generic responses that could apply uniformly across all groups as well as overly scripted or obvious answers, such as "Given I am a democrat, according to my view, ...". Finding the precise phrasing to elicit useful and varied responses required extensive experimentation.</p>
                <p><strong>Sparse Data for Each Group:</strong> The limited amount of training data available for each group made it impractical to train models like Aligner from scratch. Thus, we were focused primarily on fine-tuning existing models.</p>
                <p><strong>Handling Incomplete Sentences:</strong> We encountered data examples with incomplete sentences. These cause trouble in training aligner given it doesn't know the full question to answer as expected by the ground truth. 
                Examples include:
                <pre>{
  "key": "LEGALIMG_W41",
  "question": "In order to maintain the strength of the U.S. economy over the next 30 years, do you think that legal immigration will need to be",
  "answer": "Maintained at current levels",
  "correction": "Maintained at current levels"
},
{
  "key": "LOCALELECT_W29",
  "question": "The next question is about local elections, such as for mayor or a school board. Do you",
  "answer": "Always vote in local elections",
  "correction": "Never vote in local elections"
}</pre></p>

              <h3 class="title is-4"> Hardware and Computational Challenges</h3>
                <p><strong>Out-of-Memory Issues:</strong>
                  <ul>
                    <li>The size of the models often exceeded the available GPU memory, posing significant challenges in managing model loads and operations.</li>
                    <li>Managing the maximum generation length without exceeding memory capacities required experimentation too.</li>
                  </ul>
                </p>
                <p><strong>Exploring DeepSpeed Zero Stages:</strong> We had to experiment with different DeepSpeed Zero optimization stages to find the right balance between memory usage and computational time.</p>
                <p><strong>Precision and Efficiency:</strong> To manage these large models effectively, we loaded them in lower bit resolutions (e.g., using 16-bit precision) for more memory-efficient training, while maintaining calculations in 32-bit to preserve accuracy.</p>
                <!-- <p><strong>Necessity of LoRA:</strong> Implementing Low-Rank Adaptations (LoRA) was essential to modify large models without extensively retraining them, thus saving on computational resources.</p> -->
                <!-- IS BELOW NECESSARY??
                Multiple Codebases and Documentation Gaps: Integrating multiple codebases and navigating incomplete documentation, especially for specific training details in models like Aligner and DeepSpeed's configurations, added complexity to our development process. -->
            </div>
          
<hr>
          <h2 class="title is-3">Future Work</h2>
            <div class="content has-text-justified">
              <ul>
                <li><strong>Enhanced Evaluation Metrics:</strong> Incorporate language models to supplement the ROUGE metrics for a more comprehensive evaluation of model performance.</li>
                <li><strong>Data Diversity and Scale:</strong> Develop methodologies to automatically generate additional questions based on existing datasets, thereby enriching the dataset to cover a broader range of group preferences. Investigate the impact of utilizing larger and more varied datasets to enhance the robustness and generalizability of our findings. </li>
                <li><strong>Other Alignment Strategies:</strong> Experiment with techniques to perform a direct one-stage group preference alignment and compare its efficiency against the two-stage approach proposed in our study. Limited dataset size has previously restricted this analysis, but with access to more comprehensive data, a detailed comparison could be achievable. We can also experiment with PPO in the initial stage of GPO model training to make our comparative analysis thorough.</li>
                <li><strong>Individual Preference Modeling:</strong> We will also focus on adapting our approach to model individual preferences. This will involve developing innovative architectures and creating new datasets from user interactions to better understand and align to individual needs.</li>
              </ul>
            </div>
<hr>
      </div>
    </div>
  </div>
</section>

<section class="section" id="References">
  <div class="container is-max-desktop content">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
      <h2 class="title">References</h2>
      <ol>
        <li id="gpo">Zhao, Siyan, John Dang, and Aditya Grover. "Group preference optimization: Few-shot alignment of large language models." arXiv preprint arXiv:2310.11523 (2023).</li>
        <li id="aligner">Ji, Jiaming, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, and Yaodong Yang. "Aligner: Achieving efficient alignment through weak-to-strong correction." arXiv preprint arXiv:2402.02416 (2024).</li>
        <li id="opinionqa">Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. "Whose opinions do language models reflect?" arXiv preprint arXiv:2303.17548 (2023).</li>
        <li id="aligner-7b"><a href="https://huggingface.co/aligner/aligner-7b-v1.0">aligner-7b-v1.0</a></li>
        <li id="aligner-20k"><a href="https://huggingface.co/datasets/aligner/aligner-20K">aligner-20K</a></li>
        <li id="dpo">Rafailov, Rafael, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. "Direct preference optimization: Your language model is secretly a reward model." Advances in Neural Information Processing Systems 36 (2024).</li>
        <li id="ppo">Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. "Proximal policy optimization algorithms." arXiv preprint arXiv:1707.06347 (2017).</li>
        <li id="aligner-code"><a href="https://github.com/Aligner2024/aligner">Aligner Code</a></li>
        <li id="gpo-code"><a href="https://github.com/jamqd/Group-Preference-Optimization">Group Preference Optimization Code</a></li>
      </ol>
    </div>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
